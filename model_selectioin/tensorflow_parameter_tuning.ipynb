{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "development = pd.read_csv(\"./../../DSL_Winter_Project_2024/development.csv\")\n",
    "\n",
    "outlier_column_index=[0, 7, 12, 15, 16, 17]\n",
    "columns_to_drop=[]\n",
    "\n",
    "for index in outlier_column_index:\n",
    "    columns_to_drop.append('pmax[%s]' % index)\n",
    "    columns_to_drop.append('negpmax[%s]' % index)\n",
    "    columns_to_drop.append('tmax[%s]' % index)\n",
    "    columns_to_drop.append('area[%s]' % index)\n",
    "    columns_to_drop.append('rms[%s]' % index)\n",
    "\n",
    "evaluation = pd.read_csv(\"./../../DSL_Winter_Project_2024/evaluation.csv\")\n",
    "eva=evaluation.drop(columns=columns_to_drop)\n",
    "eva_df=eva.drop(columns=[\"Id\"])\n",
    "\n",
    "df=development.drop(columns=columns_to_drop)\n",
    "X=df.drop(columns=['x', 'y'])\n",
    "y=df.loc[:,['x', 'y']]\n",
    "\n",
    "rs=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    for i in range(hp.Int('num_layers', min_value=1, max_value=5, step=1)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32),\n",
    "                               activation='sigmoid'))\n",
    "        model.add(layers.Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(units=2))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae', 'acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    overwrite=True,\n",
    "    directory='my_tuning_directory',\n",
    "    project_name='my_first_tuning_project'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 03m 30s]\n",
      "val_loss: 64.61798477172852\n",
      "\n",
      "Best val_loss So Far: 38.04619789123535\n",
      "Total elapsed time: 00h 51m 20s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_layers': 3,\n",
       " 'units_0': 64,\n",
       " 'dropout_0': 0.1,\n",
       " 'learning_rate': 0.00012162727501515752,\n",
       " 'units_1': 32,\n",
       " 'dropout_1': 0.0,\n",
       " 'units_2': 448,\n",
       " 'dropout_2': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7710/7710 [==============================] - 7s 892us/step - loss: 116855.5000 - mae: 317.6105 - acc: 0.5209 - val_loss: 68227.0859 - val_mae: 233.4563 - val_acc: 0.5218\n",
      "Epoch 2/50\n",
      "7710/7710 [==============================] - 7s 856us/step - loss: 37385.9648 - mae: 159.8214 - acc: 0.4982 - val_loss: 17377.2930 - val_mae: 110.5212 - val_acc: 0.5218\n",
      "Epoch 3/50\n",
      "7710/7710 [==============================] - 7s 858us/step - loss: 12354.1582 - mae: 92.6348 - acc: 0.6011 - val_loss: 6243.1479 - val_mae: 63.1730 - val_acc: 0.9757\n",
      "Epoch 4/50\n",
      "7710/7710 [==============================] - 7s 862us/step - loss: 3544.8650 - mae: 42.8766 - acc: 0.9842 - val_loss: 1538.3059 - val_mae: 25.3227 - val_acc: 0.9843\n",
      "Epoch 5/50\n",
      "7710/7710 [==============================] - 7s 894us/step - loss: 660.7171 - mae: 14.4470 - acc: 0.9845 - val_loss: 191.6237 - val_mae: 7.5354 - val_acc: 0.9849\n",
      "Epoch 6/50\n",
      "7710/7710 [==============================] - 7s 895us/step - loss: 83.6166 - mae: 5.3404 - acc: 0.9855 - val_loss: 46.6792 - val_mae: 4.3630 - val_acc: 0.9856\n",
      "Epoch 7/50\n",
      "7710/7710 [==============================] - 7s 885us/step - loss: 31.2243 - mae: 3.8467 - acc: 0.9864 - val_loss: 31.4530 - val_mae: 3.4928 - val_acc: 0.9872\n",
      "Epoch 8/50\n",
      "7710/7710 [==============================] - 7s 923us/step - loss: 25.1839 - mae: 3.5459 - acc: 0.9871 - val_loss: 31.2078 - val_mae: 3.7432 - val_acc: 0.9866\n",
      "Epoch 9/50\n",
      "7710/7710 [==============================] - 7s 890us/step - loss: 22.6187 - mae: 3.3902 - acc: 0.9874 - val_loss: 26.8935 - val_mae: 3.2231 - val_acc: 0.9888\n",
      "Epoch 10/50\n",
      "7710/7710 [==============================] - 7s 858us/step - loss: 21.3573 - mae: 3.3122 - acc: 0.9875 - val_loss: 25.6503 - val_mae: 3.2303 - val_acc: 0.9884\n",
      "Epoch 11/50\n",
      "7710/7710 [==============================] - 7s 870us/step - loss: 20.2210 - mae: 3.2493 - acc: 0.9879 - val_loss: 24.8101 - val_mae: 3.2721 - val_acc: 0.9877\n",
      "Epoch 12/50\n",
      "7710/7710 [==============================] - 7s 881us/step - loss: 19.0804 - mae: 3.1901 - acc: 0.9882 - val_loss: 23.4902 - val_mae: 3.2281 - val_acc: 0.9878\n",
      "Epoch 13/50\n",
      "7710/7710 [==============================] - 7s 882us/step - loss: 18.4808 - mae: 3.1479 - acc: 0.9882 - val_loss: 22.8137 - val_mae: 3.1912 - val_acc: 0.9886\n",
      "Epoch 14/50\n",
      "7710/7710 [==============================] - 7s 854us/step - loss: 17.7679 - mae: 3.1101 - acc: 0.9883 - val_loss: 22.0306 - val_mae: 3.1302 - val_acc: 0.9887\n",
      "Epoch 15/50\n",
      "7710/7710 [==============================] - 8s 981us/step - loss: 17.3003 - mae: 3.0768 - acc: 0.9883 - val_loss: 21.7224 - val_mae: 3.1125 - val_acc: 0.9902\n",
      "Epoch 16/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 16.7928 - mae: 3.0409 - acc: 0.9884 - val_loss: 21.4460 - val_mae: 3.0486 - val_acc: 0.9883\n",
      "Epoch 17/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 16.5707 - mae: 3.0193 - acc: 0.9883 - val_loss: 22.0468 - val_mae: 3.0823 - val_acc: 0.9893\n",
      "Epoch 18/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 16.0107 - mae: 2.9849 - acc: 0.9885 - val_loss: 20.7586 - val_mae: 3.0566 - val_acc: 0.9879\n",
      "Epoch 19/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 15.7401 - mae: 2.9638 - acc: 0.9886 - val_loss: 22.9157 - val_mae: 3.0426 - val_acc: 0.9883\n",
      "Epoch 20/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 15.6256 - mae: 2.9500 - acc: 0.9885 - val_loss: 20.5166 - val_mae: 2.9869 - val_acc: 0.9885\n",
      "Epoch 21/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 15.3396 - mae: 2.9221 - acc: 0.9889 - val_loss: 21.5858 - val_mae: 3.0947 - val_acc: 0.9908\n",
      "Epoch 22/50\n",
      "7710/7710 [==============================] - 8s 999us/step - loss: 15.2342 - mae: 2.9105 - acc: 0.9886 - val_loss: 21.7039 - val_mae: 3.0552 - val_acc: 0.9878\n",
      "Epoch 23/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.9220 - mae: 2.8899 - acc: 0.9887 - val_loss: 21.5407 - val_mae: 2.9802 - val_acc: 0.9870\n",
      "Epoch 24/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.7806 - mae: 2.8789 - acc: 0.9889 - val_loss: 20.0599 - val_mae: 2.9417 - val_acc: 0.9880\n",
      "Epoch 25/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.5962 - mae: 2.8628 - acc: 0.9886 - val_loss: 19.9810 - val_mae: 2.9510 - val_acc: 0.9895\n",
      "Epoch 26/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.7246 - mae: 2.8426 - acc: 0.9886 - val_loss: 19.8350 - val_mae: 2.8816 - val_acc: 0.9908\n",
      "Epoch 27/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.4762 - mae: 2.8352 - acc: 0.9886 - val_loss: 21.0888 - val_mae: 2.9479 - val_acc: 0.9905\n",
      "Epoch 28/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.1072 - mae: 2.8181 - acc: 0.9888 - val_loss: 20.3802 - val_mae: 2.9967 - val_acc: 0.9898\n",
      "Epoch 29/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 14.1886 - mae: 2.8167 - acc: 0.9890 - val_loss: 20.0143 - val_mae: 2.9208 - val_acc: 0.9907\n",
      "Epoch 30/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.9076 - mae: 2.8014 - acc: 0.9890 - val_loss: 21.1528 - val_mae: 2.9057 - val_acc: 0.9910\n",
      "Epoch 31/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.8511 - mae: 2.7891 - acc: 0.9888 - val_loss: 19.3438 - val_mae: 2.8292 - val_acc: 0.9888\n",
      "Epoch 32/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.7724 - mae: 2.7790 - acc: 0.9890 - val_loss: 19.2953 - val_mae: 2.8388 - val_acc: 0.9897\n",
      "Epoch 33/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.6253 - mae: 2.7664 - acc: 0.9890 - val_loss: 20.5904 - val_mae: 2.9217 - val_acc: 0.9900\n",
      "Epoch 34/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.4381 - mae: 2.7563 - acc: 0.9891 - val_loss: 19.5242 - val_mae: 2.8673 - val_acc: 0.9895\n",
      "Epoch 35/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.3226 - mae: 2.7421 - acc: 0.9891 - val_loss: 19.6410 - val_mae: 2.9156 - val_acc: 0.9889\n",
      "Epoch 36/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.2503 - mae: 2.7395 - acc: 0.9892 - val_loss: 19.3535 - val_mae: 2.8029 - val_acc: 0.9915\n",
      "Epoch 37/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.0950 - mae: 2.7246 - acc: 0.9892 - val_loss: 19.8177 - val_mae: 2.8550 - val_acc: 0.9893\n",
      "Epoch 38/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 13.1031 - mae: 2.7198 - acc: 0.9891 - val_loss: 18.2004 - val_mae: 2.7815 - val_acc: 0.9904\n",
      "Epoch 39/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 12.8793 - mae: 2.7107 - acc: 0.9891 - val_loss: 19.9297 - val_mae: 2.8340 - val_acc: 0.9898\n",
      "Epoch 40/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 12.8169 - mae: 2.7009 - acc: 0.9892 - val_loss: 19.8478 - val_mae: 2.8532 - val_acc: 0.9895\n",
      "Epoch 41/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 12.7088 - mae: 2.6949 - acc: 0.9895 - val_loss: 20.3020 - val_mae: 2.7650 - val_acc: 0.9904\n",
      "Epoch 42/50\n",
      "7710/7710 [==============================] - 8s 1ms/step - loss: 12.6984 - mae: 2.6863 - acc: 0.9893 - val_loss: 21.4808 - val_mae: 2.8471 - val_acc: 0.9902\n",
      "Epoch 43/50\n",
      "7710/7710 [==============================] - 7s 905us/step - loss: 12.5064 - mae: 2.6754 - acc: 0.9894 - val_loss: 19.4539 - val_mae: 2.8043 - val_acc: 0.9891\n",
      "Epoch 44/50\n",
      "7710/7710 [==============================] - 7s 845us/step - loss: 12.4072 - mae: 2.6676 - acc: 0.9891 - val_loss: 19.8000 - val_mae: 2.8084 - val_acc: 0.9894\n",
      "Epoch 45/50\n",
      "7710/7710 [==============================] - 7s 867us/step - loss: 12.5084 - mae: 2.6607 - acc: 0.9894 - val_loss: 21.5409 - val_mae: 2.7926 - val_acc: 0.9899\n",
      "Epoch 46/50\n",
      "7710/7710 [==============================] - 7s 884us/step - loss: 12.3568 - mae: 2.6584 - acc: 0.9895 - val_loss: 20.2297 - val_mae: 2.8492 - val_acc: 0.9889\n",
      "Epoch 47/50\n",
      "7710/7710 [==============================] - 7s 875us/step - loss: 12.3901 - mae: 2.6557 - acc: 0.9894 - val_loss: 20.4533 - val_mae: 2.8155 - val_acc: 0.9896\n",
      "Epoch 48/50\n",
      "7710/7710 [==============================] - 7s 860us/step - loss: 12.2502 - mae: 2.6445 - acc: 0.9894 - val_loss: 20.1028 - val_mae: 2.8299 - val_acc: 0.9901\n",
      "Epoch 49/50\n",
      "7710/7710 [==============================] - 7s 874us/step - loss: 12.1595 - mae: 2.6346 - acc: 0.9897 - val_loss: 17.8096 - val_mae: 2.7826 - val_acc: 0.9894\n",
      "Epoch 50/50\n",
      "7710/7710 [==============================] - 7s 871us/step - loss: 12.2622 - mae: 2.6345 - acc: 0.9897 - val_loss: 22.0888 - val_mae: 2.8556 - val_acc: 0.9895\n",
      "2410/2410 [==============================] - 1s 366us/step\n",
      "4.462066137530622\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def build_model(activation='sigmoid', optimizer='adam'):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(512, activation=activation),\n",
    "        layers.Dense(128, activation=activation),\n",
    "        layers.Dense(64, activation=activation),\n",
    "        layers.Dense(32, activation=activation),\n",
    "        layers.Dense(2)\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "distance=np.mean(np.sqrt(np.sum((y_pred - y_test) ** 2, axis=1)))\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_df_scaled = scaler.transform(eva_df)\n",
    "\n",
    "eva_y=model.predict(eva_df_scaled)\n",
    "\n",
    "df = pd.DataFrame(eva_y, columns=['Id', 'Predicted'])\n",
    "df['Predicted'] = df.apply(lambda row: f\"{row['Id']:.1f}|{row['Predicted']:.1f}\", axis=1)\n",
    "df['Id'] = df.index\n",
    "df.to_csv('tensor_output4.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
